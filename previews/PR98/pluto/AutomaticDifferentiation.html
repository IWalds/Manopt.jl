<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>AD in Manopt · Manopt.jl</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../index.html"><img src="../assets/logo.png" alt="Manopt.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit">Manopt.jl</span></div><form class="docs-search" action="../search.html"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../index.html">Home</a></li><li><a class="tocitem" href="../about.html">About</a></li><li><span class="tocitem">How to...</span><ul><li><a class="tocitem" href="../tutorials/MeanAndMedian.html">get Started: Optimize!</a></li><li><a class="tocitem" href="../tutorials/Benchmark.html">speed up! using <code>gradF!</code></a></li><li><a class="tocitem" href="../tutorials/GeodesicRegression.html">Do Geodesic regression</a></li><li><a class="tocitem" href="../tutorials/HowToRecord.html">Record values</a></li><li><a class="tocitem" href="../tutorials/StochasticGradientDescent.html">do stochastic gradient descent</a></li><li><a class="tocitem" href="../tutorials/BezierCurves.html">work with Bézier curves</a></li><li><a class="tocitem" href="../tutorials/GradientOfSecondOrderDifference.html">see the gradient of <span>$d_2$</span></a></li><li><a class="tocitem" href="../tutorials/JacobiFields.html">use Jacobi Fields</a></li><li class="is-active"><a class="tocitem" href="AutomaticDifferentiation.html">AD in Manopt</a></li></ul></li><li><a class="tocitem" href="../plans/index.html">Plans</a></li><li><span class="tocitem">Solvers</span><ul><li><a class="tocitem" href="../solvers/index.html">Introduction</a></li><li><a class="tocitem" href="../solvers/alternating_gradient_descent.html">Alternating Gradient Descent</a></li><li><a class="tocitem" href="../solvers/ChambollePock.html">Chambolle-Pock</a></li><li><a class="tocitem" href="../solvers/conjugate_gradient_descent.html">Conjugate gradient descent</a></li><li><a class="tocitem" href="../solvers/cyclic_proximal_point.html">Cyclic Proximal Point</a></li><li><a class="tocitem" href="../solvers/DouglasRachford.html">Douglas–Rachford</a></li><li><a class="tocitem" href="../solvers/gradient_descent.html">Gradient Descent</a></li><li><a class="tocitem" href="../solvers/NelderMead.html">Nelder–Mead</a></li><li><a class="tocitem" href="../solvers/particle_swarm.html">Particle Swarm Optimization</a></li><li><a class="tocitem" href="../solvers/quasi_Newton.html">Quasi-Newton</a></li><li><a class="tocitem" href="../solvers/stochastic_gradient_descent.html">Stochastic Gradient Descent</a></li><li><a class="tocitem" href="../solvers/subgradient.html">Subgradient method</a></li><li><a class="tocitem" href="../solvers/truncated_conjugate_gradient_descent.html">Steihaug-Toint TCG Method</a></li><li><a class="tocitem" href="../solvers/trust_regions.html">Trust-Regions Solver</a></li></ul></li><li><span class="tocitem">Functions</span><ul><li><a class="tocitem" href="../functions/index.html">Introduction</a></li><li><a class="tocitem" href="../functions/bezier.html">Bézier curves</a></li><li><a class="tocitem" href="../functions/costs.html">Cost functions</a></li><li><a class="tocitem" href="../functions/differentials.html">Differentials</a></li><li><a class="tocitem" href="../functions/adjointdifferentials.html">Adjoint Differentials</a></li><li><a class="tocitem" href="../functions/gradients.html">Gradients</a></li><li><a class="tocitem" href="../functions/Jacobi_fields.html">Jacobi Fields</a></li><li><a class="tocitem" href="../functions/proximal_maps.html">Proximal Maps</a></li><li><a class="tocitem" href="../functions/manifold.html">Specific Manifold Functions</a></li></ul></li><li><span class="tocitem">Helpers</span><ul><li><a class="tocitem" href="../helpers/data.html">Data</a></li><li><a class="tocitem" href="../helpers/errorMeasures.html">Error Measures</a></li><li><a class="tocitem" href="../helpers/exports.html">Exports</a></li></ul></li><li><a class="tocitem" href="../contributing.html">Contributing to Manopt.jl</a></li><li><a class="tocitem" href="../notation.html">Notation</a></li><li><a class="tocitem" href="../list.html">Function Index</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">How to...</a></li><li class="is-active"><a href="AutomaticDifferentiation.html">AD in Manopt</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href="AutomaticDifferentiation.html">AD in Manopt</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/JuliaManifolds/Manopt.jl/blob/master/docs/src/pluto/AutomaticDifferentiation.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><pre><code class="language-none">  ```@meta
  EditURL = &quot;/home/runner/work/Manopt.jl/Manopt.jl/docs/../pluto/AutomaticDifferentiation.jl&quot;
  ```
  ```@raw html
  &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/gh/fonsp/Pluto.jl@0.16.4/frontend/treeview.css&quot; type=&quot;text/css&quot; /&gt;
  &lt;style&gt;
  div.markdown {
      padding-top: 1rem;
      padding-bottom: 2rem;
  }
  &lt;/style&gt;</code></pre><p>&lt;div class=&quot;markdown&quot;&gt;&lt;h1&gt;Using &amp;#40;Euclidean&amp;#41; AD in Manopt.jl&lt;/h1&gt; &lt;/div&gt;</p><p>&lt;div class=&quot;markdown&quot;&gt;&lt;p&gt;Since &lt;a href=&quot;https://juliamanifolds.github.io/Manifolds.jl/latest/&quot;&gt;Manifolds.jl&lt;/a&gt; 0.7 the support of automatic differentiation support has been extended.&lt;/p&gt; &lt;p&gt;This tutorial explains how to use Euclidean tools to derive a gradient for a real-valued function &lt;span class=&quot;tex&quot;&gt;<span>$F\colon \mathcal M → ℝ$</span>&lt;/span&gt;. We will consider two methods: an intrinsic variant and a variant employing the embedding. These gradients can then be used within any gradient based optimisation algorithm in &lt;a href=&quot;https://manoptjl.org&quot;&gt;Manopt.jl&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;While by default we use &lt;a href=&quot;https://juliadiff.org/FiniteDifferences.jl/latest/&quot;&gt;FiniteDifferences.jl&lt;/a&gt;, you can also use &lt;a href=&quot;https://github.com/JuliaDiff/FiniteDiff.jl&quot;&gt;FiniteDiff.jl&lt;/a&gt;, &lt;a href=&quot;https://juliadiff.org/ForwardDiff.jl/stable/&quot;&gt;ForwardDiff.jl&lt;/a&gt;, &lt;a href=&quot;https://juliadiff.org/ReverseDiff.jl/&quot;&gt;ReverseDiff.jl&lt;/a&gt;, or  &lt;a href=&quot;https://fluxml.ai/Zygote.jl/&quot;&gt;Zygote.jl&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt;</p><p>&lt;div class=&quot;markdown&quot;&gt;&lt;p&gt;In this Notebook we will take a look at a few possibilities to approximate or derive the gradient of a function &lt;span class=&quot;tex&quot;&gt;<span>$f:\mathcal M \to ℝ$</span>&lt;/span&gt; on a Riemannian manifold, without computing it yourself. There is mainly two different philosophies:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Working &lt;em&gt;instrinsically&lt;/em&gt;, i.e. stay on the manifold and in the tangent spaces. Here, we will consider approximating the gradient by forward differences.&lt;/p&gt; &lt;/li&gt; &lt;li&gt;&lt;p&gt;Working in an embedding – there we can use all tools from functions on Euclidean spaces – finite differences or automatic differenciation – and then compute the corresponding Riemannian gradient from there.&lt;/p&gt; &lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Let&amp;#39;s first load all packages we need.&lt;/p&gt; &lt;/div&gt;</p><p>&lt;pre&gt;&lt;code class=&quot;language-julia&quot;&gt;using Manifolds, Manopt, Random, LinearAlgebra&lt;/code&gt;&lt;/pre&gt;</p><p>&lt;pre&gt;&lt;code class=&quot;language-julia&quot;&gt;using FiniteDiff, ReverseDiff&lt;/code&gt;&lt;/pre&gt;</p><p>&lt;div class=&quot;markdown&quot;&gt;&lt;h2&gt;1. &amp;#40;Intrinsic&amp;#41; Forward Differences&lt;/h2&gt; &lt;p&gt;A first idea is to generalise &amp;#40;multivariate&amp;#41; finite differences to Riemannian manifolds. Let &lt;span class=&quot;tex&quot;&gt;<span>$X_1,\ldots,X_d ∈ T_p\mathcal M$</span>&lt;/span&gt; denote an orthonormal basis of the tangent space &lt;span class=&quot;tex&quot;&gt;<span>$T_p\mathcal M$</span>&lt;/span&gt; at the point &lt;span class=&quot;tex&quot;&gt;<span>$p∈\mathcal M$</span>&lt;/span&gt; on the Riemannian manifold.&lt;/p&gt; &lt;p&gt;We can generalise the notion of a directional derivative, i.e. for the “direction” &lt;span class=&quot;tex&quot;&gt;<span>$Y∈T_p\mathcal M$</span>&lt;/span&gt; let &lt;span class=&quot;tex&quot;&gt;<span>$c\colon &amp;#91;-ε,ε&amp;#93;$</span>&lt;/span&gt;, &lt;span class=&quot;tex&quot;&gt;<span>$ε&amp;gt;0$</span>&lt;/span&gt;, be a curve with &lt;span class=&quot;tex&quot;&gt;<span>$c&amp;#40;0&amp;#41; &amp;#61; p$</span>&lt;/span&gt;, &lt;span class=&quot;tex&quot;&gt;<span>$\dot c&amp;#40;0&amp;#41; &amp;#61; Y$</span>&lt;/span&gt; and we obtain&lt;/p&gt; &lt;p class=&quot;tex&quot;&gt;$	Df&amp;#40;p&amp;#41;&amp;#91;Y&amp;#93; &amp;#61; \frac&amp;#123;\mathrm&amp;#123;d&amp;#125;&amp;#125;&amp;#123;\mathrm&amp;#123;d&amp;#125;t&amp;#125; f&amp;#40;c&amp;#40;t&amp;#41;&amp;#41; &amp;#61; \lim<em>&amp;#123;h \to 0&amp;#125; \frac&amp;#123;1&amp;#125;&amp;#123;h&amp;#125;&amp;#40;f&amp;#40;\exp</em>p&amp;#40;hY&amp;#41;&amp;#41;-f&amp;#40;p&amp;#41;&amp;#41;<span>$&lt;/p&gt; &lt;p&gt;We can approximate &lt;span class=&quot;tex&quot;&gt;$Df&amp;#40;p&amp;#41;&amp;#91;X&amp;#93;$&lt;/span&gt; by a finite difference scheme for an &lt;span class=&quot;tex&quot;&gt;$h&amp;gt;0$&lt;/span&gt; as&lt;/p&gt; &lt;p class=&quot;tex&quot;&gt;$</span>DF&amp;#40;p&amp;#41;&amp;#91;Y&amp;#93; ≈ G<em>h&amp;#40;Y&amp;#41; :&amp;#61; \frac&amp;#123;1&amp;#125;&amp;#123;h&amp;#125;&amp;#40;f&amp;#40;\exp</em>p&amp;#40;hY&amp;#41;&amp;#41;-f&amp;#40;p&amp;#41;&amp;#41;<span>$&lt;/p&gt; &lt;p&gt;Furthermore the gradient &lt;span class=&quot;tex&quot;&gt;$\operatorname&amp;#123;grad&amp;#125;f$&lt;/span&gt; is the Riesz representer of the differential, ie.&lt;/p&gt; &lt;p class=&quot;tex&quot;&gt;$</span>	Df&amp;#40;p&amp;#41;&amp;#91;Y&amp;#93; &amp;#61; g<em>p&amp;#40;\operatorname&amp;#123;grad&amp;#125;f&amp;#40;p&amp;#41;, Y&amp;#41;,\qquad \text&amp;#123; for all &amp;#125; Y ∈ T</em>p\mathcal M<span>$&lt;/p&gt; &lt;p&gt;and since it is a tangent vector, we can write it in terms of a basis as&lt;/p&gt; &lt;p class=&quot;tex&quot;&gt;$</span>	\operatorname&amp;#123;grad&amp;#125;f&amp;#40;p&amp;#41; &amp;#61; \sum<em>&amp;#123;i&amp;#61;1&amp;#125;^&amp;#123;d&amp;#125; g</em>p&amp;#40;\operatorname&amp;#123;grad&amp;#125;f&amp;#40;p&amp;#41;,X<em>i&amp;#41;X</em>i 	&amp;#61; \sum<em>&amp;#123;i&amp;#61;1&amp;#125;^&amp;#123;d&amp;#125; Df&amp;#40;p&amp;#41;&amp;#91;X</em>i&amp;#93;X<em>i<span>$&lt;/p&gt; &lt;p&gt;and perform the approximation from above to obtain&lt;/p&gt; &lt;p class=&quot;tex&quot;&gt;$</span>	\operatorname&amp;#123;grad&amp;#125;f&amp;#40;p&amp;#41; ≈ \sum</em>&amp;#123;i&amp;#61;1&amp;#125;^&amp;#123;d&amp;#125; G<em>h&amp;#40;X</em>i&amp;#41;X_i$&lt;/p&gt; &lt;p&gt;for some suitable step size &lt;span class=&quot;tex&quot;&gt;<span>$h$</span>&lt;/span&gt;.This comes at the cost of &lt;span class=&quot;tex&quot;&gt;<span>$d&amp;#43;1$</span>&lt;/span&gt; function evaluations and &lt;span class=&quot;tex&quot;&gt;<span>$d$</span>&lt;/span&gt; exponential maps.&lt;/p&gt; &lt;/div&gt;</p><p>&lt;div class=&quot;markdown&quot;&gt;&lt;p&gt;This is the first variant we can use. An advantage is, that it is &lt;em&gt;intrinsic&lt;/em&gt; in the sense that it does not require any embedding of the manifold.&lt;/p&gt; &lt;/div&gt;</p><p>&lt;div class=&quot;markdown&quot;&gt;&lt;h3&gt;An Example: The Rayleigh Quotient&lt;/h3&gt; &lt;p&gt;The Rayleigh quotient is concerned with finding Eigenvalues &amp;#40;and Eigenvectors&amp;#41; of a symmetric matrix &lt;span class=&quot;tex&quot;&gt;<span>$A\in ℝ^&amp;#123;&amp;#40;n&amp;#43;1&amp;#41;×&amp;#40;n&amp;#43;1&amp;#41;&amp;#125;$</span>&lt;/span&gt;. The optimisation problem reads&lt;/p&gt; &lt;p class=&quot;tex&quot;&gt;<span>$F\colon ℝ^&amp;#123;n&amp;#43;1&amp;#125; \to ℝ,\quad F&amp;#40;\mathbf x&amp;#41; &amp;#61; \frac&amp;#123;\mathbf x^\mathrm&amp;#123;T&amp;#125;A\mathbf x&amp;#125;&amp;#123;\mathbf x^\mathrm&amp;#123;T&amp;#125;\mathbf x&amp;#125;$</span>&lt;/p&gt; &lt;p&gt;Minimizing this function yields the smallest eigenvalue &lt;span class=&quot;tex&quot;&gt;<span>$\lambda_1$</span>&lt;/span&gt; as a value and the corresponding minimizer &lt;span class=&quot;tex&quot;&gt;<span>$\mathbf x^*$</span>&lt;/span&gt; is a corresponding eigenvector.&lt;/p&gt; &lt;p&gt;Since the length of an eigenvector is irrelevant, there is an ambiguity in the cost function. It can be better phrased on the sphere &lt;span class=&quot;tex&quot;&gt;<span>$𝕊^n$</span>&lt;/span&gt; of unit vectors in &lt;span class=&quot;tex&quot;&gt;<span>$\mathbb R^&amp;#123;n&amp;#43;1&amp;#125;$</span>&lt;/span&gt;, i.e.&lt;/p&gt; &lt;p class=&quot;tex&quot;&gt;<span>$\operatorname*&amp;#123;arg\,min&amp;#125;_&amp;#123;p \in 𝕊^n&amp;#125; f&amp;#40;p&amp;#41; &amp;#61; \operatorname*&amp;#123;arg\,min&amp;#125;_&amp;#123;p \in 𝕊^n&amp;#125; p^\mathrm&amp;#123;T&amp;#125;Ap$</span>&lt;/p&gt; &lt;p&gt;We can compute the Riemannian gradient exactly as&lt;/p&gt; &lt;p class=&quot;tex&quot;&gt;<span>$\operatorname&amp;#123;grad&amp;#125; f&amp;#40;p&amp;#41; &amp;#61; 2&amp;#40;Ap - pp^\mathrm&amp;#123;T&amp;#125;Ap&amp;#41;$</span>&lt;/p&gt; &lt;p&gt;so we can compare it to the approximation by finite differences.&lt;/p&gt; &lt;/div&gt;</p><p>&lt;pre&gt;&lt;code class=&quot;language-julia&quot;&gt;begin     Random.seed!(42)     n = 200     A = randn(n + 1, n + 1)     A = Symmetric(A)     M = Sphere(n)     nothing end&lt;/code&gt;&lt;/pre&gt;</p><p>&lt;pre&gt;&lt;code class=&quot;language-julia&quot;&gt;f1(p) = p&#39; * A&#39;p&lt;/code&gt;&lt;/pre&gt; &lt;pre&gt;&lt;code class=&quot;code-output&quot;&gt;f1 (generic function with 1 method)&lt;/code&gt;&lt;/pre&gt;</p><p>&lt;pre&gt;&lt;code class=&quot;language-julia&quot;&gt;gradf1(p) = 2 * (A * p - p * p&#39; * A * p)&lt;/code&gt;&lt;/pre&gt; &lt;pre&gt;&lt;code class=&quot;code-output&quot;&gt;gradf1 (generic function with 1 method)&lt;/code&gt;&lt;/pre&gt;</p><p>&lt;div class=&quot;markdown&quot;&gt;&lt;p&gt;Manifolds provides a finite difference scheme in Tangent spaces, that you can introduce to use an existing framework &amp;#40;if the wrapper is implemented&amp;#41; form Euclidean space. Here we use &lt;code&gt;FiniteDiff.jl&lt;/code&gt;.&lt;/p&gt; &lt;/div&gt;</p><p>&lt;pre&gt;&lt;code class=&quot;language-julia&quot;&gt;r_backend = Manifolds.TangentDiffBackend(Manifolds.FiniteDiffBackend())&lt;/code&gt;&lt;/pre&gt; &lt;pre&gt;&lt;code class=&quot;code-output&quot;&gt;TangentDiffBackend{Manifolds.FiniteDiffBackend{Val{:central}}, ExponentialRetraction, LogarithmicInverseRetraction, DefaultOrthonormalBasis{ℝ, ManifoldsBase.TangentSpaceType}}(Manifolds.FiniteDiffBackend{Val{:central}}(Val{:central}()), ExponentialRetraction(), LogarithmicInverseRetraction(), DefaultOrthonormalBasis(ℝ))&lt;/code&gt;&lt;/pre&gt;</p><p>&lt;pre&gt;&lt;code class=&quot;language-julia&quot;&gt;gradf1<em>FD(p) = Manifolds.gradient(M, f1, p, r</em>backend)&lt;/code&gt;&lt;/pre&gt; &lt;pre&gt;&lt;code class=&quot;code-output&quot;&gt;gradf1_FD (generic function with 1 method)&lt;/code&gt;&lt;/pre&gt;</p><p>&lt;pre&gt;&lt;code class=&quot;language-julia&quot;&gt;begin     p = zeros(n + 1)     p[1] = 1.0     X1 = gradf1(p)     X2 = gradf1_FD(p)     norm(M, p, X1 - X2) end&lt;/code&gt;&lt;/pre&gt; &lt;pre&gt;&lt;code class=&quot;code-output&quot;&gt;7.076144686543836e-10&lt;/code&gt;&lt;/pre&gt;</p><p>&lt;div class=&quot;markdown&quot;&gt;&lt;p&gt;We obtain quite a good approximation of the gradient.&lt;/p&gt; &lt;/div&gt;</p><p>&lt;div class=&quot;markdown&quot;&gt;&lt;h2&gt;2. Conversion of an Euclidean Gradient in the Embedding to a Riemannian Gradient of an &amp;#40;not necessarily isometrically&amp;#41; embedded Manifold&lt;/h2&gt; &lt;p&gt;Let &lt;span class=&quot;tex&quot;&gt;<span>$\tilde f\colon\mathbb R^m \to \mathbb R$</span>&lt;/span&gt; be a function in the embedding of an &lt;span class=&quot;tex&quot;&gt;<span>$n$</span>&lt;/span&gt;-dimensional manifold &lt;span class=&quot;tex&quot;&gt;<span>$\mathcal M \subset \mathbb R^m$</span>&lt;/span&gt; and &lt;span class=&quot;tex&quot;&gt;<span>$f\colon \mathcal M \to \mathbb R$</span>&lt;/span&gt; denote the restriction of &lt;span class=&quot;tex&quot;&gt;<span>$\tilde f$</span>&lt;/span&gt; to the manifold &lt;span class=&quot;tex&quot;&gt;<span>$\mathcal M$</span>&lt;/span&gt;.&lt;/p&gt; &lt;p&gt;Since we can use the push forward of the embedding to also embed the tangent space &lt;span class=&quot;tex&quot;&gt;<span>$T_p\mathcal M$</span>&lt;/span&gt;, &lt;span class=&quot;tex&quot;&gt;<span>$p\in \mathcal M$</span>&lt;/span&gt;, we can similarly obtain the differential &lt;span class=&quot;tex&quot;&gt;<span>$Df&amp;#40;p&amp;#41;\colon T_p\mathcal M \to \mathbb R$</span>&lt;/span&gt; by restricting the differential &lt;span class=&quot;tex&quot;&gt;<span>$D\tilde f&amp;#40;p&amp;#41;$</span>&lt;/span&gt; to the tangent space.&lt;/p&gt; &lt;p&gt;If both &lt;span class=&quot;tex&quot;&gt;<span>$T_p\mathcal M$</span>&lt;/span&gt; and &lt;span class=&quot;tex&quot;&gt;<span>$T_p\mathcal R^m$</span>&lt;/span&gt; have the same inner product, or in other words the manifold is isometrically embedded in &lt;span class=&quot;tex&quot;&gt;<span>$R^m$</span>&lt;/span&gt; &amp;#40;like for example the sphere &lt;span class=&quot;tex&quot;&gt;<span>$\mathbb S^n\subset\mathbb R^&amp;#123;m&amp;#43;1&amp;#125;$</span>&lt;/span&gt; then this restriction of the differential directly translates to a projection of the gradient, i.e.&lt;/p&gt; &lt;p class=&quot;tex&quot;&gt;<span>$\operatorname&amp;#123;grad&amp;#125;f&amp;#40;p&amp;#41; &amp;#61; \operatorname&amp;#123;Proj&amp;#125;_&amp;#123;T_p\mathcal M&amp;#125;&amp;#40;\operatorname&amp;#123;grad&amp;#125; \tilde f&amp;#40;p&amp;#41;&amp;#41;$</span>&lt;/p&gt; &lt;p&gt;More generally we might have to take a change of the metric into account, i.e.&lt;/p&gt; &lt;p class=&quot;tex&quot;&gt;<span>$\langle  \operatorname&amp;#123;Proj&amp;#125;_&amp;#123;T_p\mathcal M&amp;#125;&amp;#40;\operatorname&amp;#123;grad&amp;#125; \tilde f&amp;#40;p&amp;#41;&amp;#41;, X \rangle &amp;#61; Df&amp;#40;p&amp;#41;&amp;#91;X&amp;#93; &amp;#61; g_p&amp;#40;\operatorname&amp;#123;grad&amp;#125;f&amp;#40;p&amp;#41;, X&amp;#41;$</span>&lt;/p&gt; &lt;p&gt;or in words: we have to change the Riesz representer of the &amp;#40;restricted/projected&amp;#41; differential of &lt;span class=&quot;tex&quot;&gt;<span>$f$</span>&lt;/span&gt; &amp;#40;&lt;span class=&quot;tex&quot;&gt;<span>$\tilde f$</span>&lt;/span&gt;&amp;#41; to the one with respect to the Riemannian metric. This is done using &lt;a href=&quot;https://juliamanifolds.github.io/Manifolds.jl/latest/manifolds/metric.html#Manifolds.change<em>representer-Tuple&amp;#123;AbstractManifold,&amp;#37;20AbstractMetric,&amp;#37;20Any,&amp;#37;20Any&amp;#125;&quot;&gt;&lt;code&gt;change</em>representer&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt;</p><p>&lt;div class=&quot;markdown&quot;&gt;&lt;h3&gt;A continued Example&lt;/h3&gt; &lt;p&gt;We continue with the Rayleigh Quotient from before, now just starting with the defintion of the Euclidean case in the embedding, the function &lt;span class=&quot;tex&quot;&gt;<span>$F$</span>&lt;/span&gt;.&lt;/p&gt; &lt;/div&gt;</p><p>&lt;pre&gt;&lt;code class=&quot;language-julia&quot;&gt;F(x) = x&#39; * A * x / (x&#39; * x);&lt;/code&gt;&lt;/pre&gt;</p><p>&lt;div class=&quot;markdown&quot;&gt;&lt;p&gt;The cost function is the same by restriction&lt;/p&gt; &lt;/div&gt;</p><p>&lt;pre&gt;&lt;code class=&quot;language-julia&quot;&gt;f2(M, p) = F(p);&lt;/code&gt;&lt;/pre&gt;</p><p>&lt;div class=&quot;markdown&quot;&gt;&lt;p&gt;The gradient is now computed combining our gradient scheme with ReverseDiff.&lt;/p&gt; &lt;/div&gt;</p><p>&lt;pre&gt;&lt;code class=&quot;language-julia&quot;&gt;function grad<em>f2</em>AD(M, p)     return Manifolds.gradient(         M, F, p, RiemannianProjectionBackend(Manifolds.ReverseDiffBackend())     ) end&lt;/code&gt;&lt;/pre&gt; &lt;pre&gt;&lt;code class=&quot;code-output&quot;&gt;grad<em>f2</em>AD (generic function with 1 method)&lt;/code&gt;&lt;/pre&gt;</p><p>&lt;pre&gt;&lt;code class=&quot;language-julia&quot;&gt;X3 = grad<em>f2</em>AD(M, p)&lt;/code&gt;&lt;/pre&gt; &lt;pre&gt;&lt;code class=&quot;code-output&quot;&gt;[0.0, 0.140385, 1.20816, -1.86501, -1.3296, 0.82336, 0.0133079, 2.12231, -0.568753, -2.69144, 0.629982, -2.69252, -0.480321, 1.50455, 0.993916, 1.78306, 1.83971, -0.637613, -0.304438, -3.94601, more, 0.807925, 1.82765, 6.21094, -0.974706, -1.08951, 2.42345, 2.34911, 0.579974, -0.0742323, 2.08591]&lt;/code&gt;&lt;/pre&gt;</p><p>&lt;pre&gt;&lt;code class=&quot;language-julia&quot;&gt;norm(M, p, X1 - X3)&lt;/code&gt;&lt;/pre&gt; &lt;pre&gt;&lt;code class=&quot;code-output&quot;&gt;0.0&lt;/code&gt;&lt;/pre&gt;</p><p>&lt;div class=&quot;markdown&quot;&gt;&lt;h3&gt;An Example for a nonisometrically embedded Manifold&lt;/h3&gt; &lt;p&gt;on the manifold &lt;span class=&quot;tex&quot;&gt;<span>$\mathcal P&amp;#40;3&amp;#41;$</span>&lt;/span&gt; of symmetric positive definite matrices.&lt;/p&gt; &lt;/div&gt;</p><p>&lt;div class=&quot;markdown&quot;&gt;&lt;p&gt;The following function computes &amp;#40;half&amp;#41; the distance squared &amp;#40;with respect to the linear affine metric&amp;#41; on the manifold &lt;span class=&quot;tex&quot;&gt;<span>$\mathcal P&amp;#40;3&amp;#41;$</span>&lt;/span&gt; to the identity, i.e. &lt;span class=&quot;tex&quot;&gt;<span>$I_3$</span>&lt;/span&gt;. denoting the unit matrix we consider the function&lt;/p&gt; &lt;p class=&quot;tex&quot;&gt;$	G&amp;#40;q&amp;#41; &amp;#61; \frac&amp;#123;1&amp;#125;&amp;#123;2&amp;#125;d^2<em>&amp;#123;\mathcal P&amp;#40;3&amp;#41;&amp;#125;&amp;#40;q,I</em>3&amp;#41; &amp;#61; \lVert \operatorname&amp;#123;Log&amp;#125;&amp;#40;q&amp;#41; \rVert<em>F^2,$&lt;/p&gt; &lt;p&gt;where &lt;span class=&quot;tex&quot;&gt;<span>$\operatorname&amp;#123;Log&amp;#125;$</span>&lt;/span&gt; denotes the matrix logarithm and &lt;span class=&quot;tex&quot;&gt;\lVert \cdot \rVert</em>F<span>$&lt;/span&gt; is the Frobenius norm. This can be computed for symmetric positive definite matrices by summing the squares of the &lt;span class=&quot;tex&quot;&gt;$</span>\log<span>$&lt;/span&gt;arithms of the eigenvalues of &lt;span class=&quot;tex&quot;&gt;$</span>q&lt;/span&gt; and divide by two:&lt;/p&gt; &lt;/div&gt;</p><p>&lt;pre&gt;&lt;code class=&quot;language-julia&quot;&gt;G(q) = sum(log.(eigvals(Symmetric(q))) .^ 2) / 2&lt;/code&gt;&lt;/pre&gt; &lt;pre&gt;&lt;code class=&quot;code-output&quot;&gt;G (generic function with 1 method)&lt;/code&gt;&lt;/pre&gt;</p><p>&lt;div class=&quot;markdown&quot;&gt;&lt;p&gt;We can also interpret this as a function on the space of matrices and apply the Euclidean finite differences machinery; in this way we can easily derive the Euclidean gradient. But when computing the Riemannian gradient, we have to change the representer &amp;#40;see again &lt;a href=&quot;https://juliamanifolds.github.io/Manifolds.jl/latest/manifolds/metric.html#Manifolds.change<em>representer-Tuple&amp;#123;AbstractManifold,&amp;#37;20AbstractMetric,&amp;#37;20Any,&amp;#37;20Any&amp;#125;&quot;&gt;&lt;code&gt;change</em>representer&lt;/code&gt;&lt;/a&gt;&amp;#41; after projecting onto the tangent space &lt;span class=&quot;tex&quot;&gt;<span>$T_p\mathcal P&amp;#40;n&amp;#41;$</span>&lt;/span&gt; at &lt;span class=&quot;tex&quot;&gt;<span>$p$</span>&lt;/span&gt;.&lt;/p&gt; &lt;p&gt;Let&amp;#39;s first define a point and the manifold &lt;span class=&quot;tex&quot;&gt;<span>$N&amp;#61;\mathcal P&amp;#40;3&amp;#41;$</span>&lt;/span&gt;.&lt;/p&gt; &lt;/div&gt;</p><p>&lt;pre&gt;&lt;code class=&quot;language-julia&quot;&gt;rotM(α) = [1.0 0.0 0.0; 0.0 cos(α) sin(α); 0.0 -sin(α) cos(α)]&lt;/code&gt;&lt;/pre&gt; &lt;pre&gt;&lt;code class=&quot;code-output&quot;&gt;rotM (generic function with 1 method)&lt;/code&gt;&lt;/pre&gt;</p><p>&lt;pre&gt;&lt;code class=&quot;language-julia&quot;&gt;q = rotM(π / 6) * [1.0 0.0 0.0; 0.0 2.0 0.0; 0.0 0.0 3.0] * transpose(rotM(π / 6))&lt;/code&gt;&lt;/pre&gt; &lt;pre&gt;&lt;code class=&quot;code-output&quot;&gt;3×3 Matrix{Float64}:  1.0  0.0       0.0  0.0  2.25      0.433013  0.0  0.433013  2.75&lt;/code&gt;&lt;/pre&gt;</p><p>&lt;pre&gt;&lt;code class=&quot;language-julia&quot;&gt;N = SymmetricPositiveDefinite(3)&lt;/code&gt;&lt;/pre&gt; &lt;pre&gt;&lt;code class=&quot;code-output&quot;&gt;SymmetricPositiveDefinite(3)&lt;/code&gt;&lt;/pre&gt;</p><p>&lt;pre&gt;&lt;code class=&quot;language-julia&quot;&gt;is_point(N, q)&lt;/code&gt;&lt;/pre&gt; &lt;pre&gt;&lt;code class=&quot;code-output&quot;&gt;true&lt;/code&gt;&lt;/pre&gt;</p><p>&lt;div class=&quot;markdown&quot;&gt;&lt;p&gt;We could first just compute the gradient using &lt;code&gt;FiniteDiff.jl&lt;/code&gt;, but this yields the Euclidean gradient:&lt;/p&gt; &lt;/div&gt;</p><p>&lt;pre&gt;&lt;code class=&quot;language-julia&quot;&gt;FiniteDiff.finite<em>difference</em>gradient(G, q)&lt;/code&gt;&lt;/pre&gt; &lt;pre&gt;&lt;code class=&quot;code-output&quot;&gt;3×3 Matrix{Float64}:  -1.83343e-11  0.0       0.0   0.0          0.351481  0.0170005   0.0          0.0       0.361296&lt;/code&gt;&lt;/pre&gt;</p><p>&lt;div class=&quot;markdown&quot;&gt;&lt;p&gt;Instead, we use the &lt;a href=&quot;https://juliamanifolds.github.io/Manifolds.jl/latest/features/differentiation.html#Manifolds.RiemannianProjectionBackend&quot;&gt;&lt;code&gt;RiemannianProjectedBackend&lt;/code&gt;&lt;/a&gt; of &lt;code&gt;Manifolds.jl&lt;/code&gt;, which in this case internally uses &lt;code&gt;FiniteDiff.jl&lt;/code&gt; to compute a Euclidean gradient but then uses the conversion explained above to derive the Riemannian gradient.&lt;/p&gt; &lt;p&gt;We define this here again as a function &lt;code&gt;grad<em>G</em>FD&lt;/code&gt; that could be used in the &lt;code&gt;Manopt.jl&lt;/code&gt; framework within a gradient based optimisation.&lt;/p&gt; &lt;/div&gt;</p><p>&lt;pre&gt;&lt;code class=&quot;language-julia&quot;&gt;function grad<em>G</em>FD(N, q)     return Manifolds.gradient(         N, G, q, RiemannianProjectionBackend(Manifolds.FiniteDiffBackend())     ) end&lt;/code&gt;&lt;/pre&gt; &lt;pre&gt;&lt;code class=&quot;code-output&quot;&gt;grad<em>G</em>FD (generic function with 1 method)&lt;/code&gt;&lt;/pre&gt;</p><p>&lt;pre&gt;&lt;code class=&quot;language-julia&quot;&gt;G1 = grad<em>G</em>FD(N, q)&lt;/code&gt;&lt;/pre&gt; &lt;pre&gt;&lt;code class=&quot;code-output&quot;&gt;3×3 Matrix{Float64}:  -1.83343e-11  0.0       0.0   0.0          1.86368   0.826856   0.0          0.826856  2.81845&lt;/code&gt;&lt;/pre&gt;</p><p>&lt;div class=&quot;markdown&quot;&gt;&lt;p&gt;Now, we can agaon compare this to the &amp;#40;known&amp;#41; solution of the gradient, namely the gradient of &amp;#40;a half&amp;#41; the distance suqared, i.e. &lt;span class=&quot;tex&quot;&gt;<span>$G&amp;#40;q&amp;#41; &amp;#61; \frac&amp;#123;1&amp;#125;&amp;#123;2&amp;#125;d^2_&amp;#123;\mathcal P&amp;#40;3&amp;#41;&amp;#125;&amp;#40;q,I_3&amp;#41;$</span>&lt;/span&gt; is given by &lt;span class=&quot;tex&quot;&gt;<span>$\operatorname&amp;#123;grad&amp;#125; G&amp;#40;q&amp;#41; &amp;#61; -\operatorname&amp;#123;log&amp;#125;_q I_3$</span>&lt;/span&gt;, where &lt;span class=&quot;tex&quot;&gt;<span>$\operatorname&amp;#123;log&amp;#125;$</span>&lt;/span&gt; is the &lt;a href=&quot;https://juliamanifolds.github.io/Manifolds.jl/latest/manifolds/symmetricpositivedefinite.html#Base.log-Tuple&amp;#123;SymmetricPositiveDefinite,&amp;#37;20Vararg&amp;#123;Any,&amp;#37;20N&amp;#125;&amp;#37;20where&amp;#37;20N&amp;#125;&quot;&gt;logarithmic map&lt;/a&gt; on the manifold.&lt;/p&gt; &lt;/div&gt;</p><p>&lt;pre&gt;&lt;code class=&quot;language-julia&quot;&gt;G2 = -log(N, q, Matrix{Float64}(I, 3, 3))&lt;/code&gt;&lt;/pre&gt; &lt;pre&gt;&lt;code class=&quot;code-output&quot;&gt;3×3 Matrix{Float64}:  -0.0  -0.0       -0.0  -0.0   1.86368    0.826856  -0.0   0.826856   2.81845&lt;/code&gt;&lt;/pre&gt;</p><p>&lt;div class=&quot;markdown&quot;&gt;&lt;p&gt;Both terms agree up to &lt;span class=&quot;tex&quot;&gt;<span>$1.2×10^&amp;#123;-10&amp;#125;$</span>&lt;/span&gt;:&lt;/p&gt; &lt;/div&gt;</p><p>&lt;pre&gt;&lt;code class=&quot;language-julia&quot;&gt;norm(G1 - G2)&lt;/code&gt;&lt;/pre&gt; &lt;pre&gt;&lt;code class=&quot;code-output&quot;&gt;1.1994981317829302e-10&lt;/code&gt;&lt;/pre&gt;</p><p>&lt;pre&gt;&lt;code class=&quot;language-julia&quot;&gt;isapprox(M, q, G1, G2; atol=2 * 1e-10)&lt;/code&gt;&lt;/pre&gt; &lt;pre&gt;&lt;code class=&quot;code-output&quot;&gt;true&lt;/code&gt;&lt;/pre&gt;</p><p>&lt;div class=&quot;markdown&quot;&gt;&lt;p&gt;In this case we can not use &lt;code&gt;ReverseDiff.jl&lt;/code&gt;, since it can not handle the &lt;code&gt;eigvals&amp;#33;&lt;/code&gt; function that is called internally.&lt;/p&gt; &lt;/div&gt;</p><p>&lt;div class=&quot;markdown&quot;&gt;&lt;h2&gt;Summary&lt;/h2&gt; &lt;p&gt;This tutorial illustrates how to use tools from Euclidean spaces, finite differences or automatic differentiation, to compute gradients on Riemannian manifolds. The scheme allows to use &lt;em&gt;any&lt;/em&gt; differentiation framework within the embedding to derive a Riemannian gradient.&lt;/p&gt; &lt;/div&gt;</p><pre><code class="language-none">  &lt;script&gt;</code></pre><p>&lt;/script&gt; &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css&quot; integrity=&quot;sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs&quot; crossorigin=&quot;anonymous&quot;&gt;</p><p>&lt;!– The loading of KaTeX is deferred to speed up page rendering –&gt;   &lt;script defer src=&quot;https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js&quot; integrity=&quot;sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx&quot; crossorigin=&quot;anonymous&quot;&gt;&lt;/script&gt;</p><p>&lt;!– To automatically render math in text elements, include the auto-render extension: –&gt;   &lt;script defer src=&quot;https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js&quot; integrity=&quot;sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR&quot; crossorigin=&quot;anonymous&quot;       onload=&quot;renderMathInElement(document.body);&quot;&gt;&lt;/script&gt;</p><p>&lt;script&gt; const options = {   delimiters: [     {left: &quot;<span>$&quot;, right: &quot;$</span>&quot;, display: true},     {left: &quot;<span>$&quot;, right: &quot;$</span>&quot;, display: false},     {left: &quot;\begin{equation}&quot;, right: &quot;\end{equation}&quot;, display: true},     {left: &quot;\begin{align}&quot;, right: &quot;\end{align}&quot;, display: true},     {left: &quot;\begin{alignat}&quot;, right: &quot;\end{alignat}&quot;, display: true},     {left: &quot;\begin{gather}&quot;, right: &quot;\end{gather}&quot;, display: true},     {left: &quot;(&quot;, right: &quot;)&quot;, display: false},     {left: &quot;[&quot;, right: &quot;]&quot;, display: true}   ] }; renderMathInElement(document.body, options); &lt;/script&gt;       &lt;/script&gt;       ```</p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../tutorials/JacobiFields.html">« use Jacobi Fields</a><a class="docs-footer-nextpage" href="../plans/index.html">Plans »</a></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Wednesday 1 December 2021 14:07">Wednesday 1 December 2021</span>. Using Julia version 1.7.0.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
